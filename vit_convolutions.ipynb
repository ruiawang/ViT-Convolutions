{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GXaRC_F-zHM"
      },
      "source": [
        "## TTIC 31230 - Fundamentals of Deep Learning Final Project\n",
        "Rui Wang\n",
        "\n",
        "My project submission and extension will be based on the [visual transformer (ViT) paper](https://arxiv.org/abs/2010.11929) under the proposed extension to incorporate convolutions in some way to the Vision Transformer, in the interest of bridging the performance and accuracy that convolution networks bring, and the efficiency of computational resources that the original Vision Transformers enjoy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j86z9NqB_gEQ"
      },
      "source": [
        "#### Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XpIC-Wl8fac",
        "outputId": "a897a3b0-c176-42f2-d760-faef453ad6c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysnDjRhuIjOT"
      },
      "outputs": [],
      "source": [
        "bs = 512\n",
        "size = 32\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzKGWtqN7sX_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oYxLF__H566"
      },
      "outputs": [],
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.Resize(size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iipdxLyt9SKy",
        "outputId": "4d072757-3ff2-4366-fbae-7a58059c9bf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "trainset = torchvision.datasets.CIFAR10(\"data\", download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuZCPhTj-iAZ",
        "outputId": "bde0e3d9-d533-49fd-b0d7-25569cb94c28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset CIFAR10\n",
              "    Number of datapoints: 50000\n",
              "    Root location: data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               RandomCrop(size=(32, 32), padding=4)\n",
              "               Resize(size=32, interpolation=bilinear, max_size=None, antialias=warn)\n",
              "               RandomHorizontalFlip(p=0.5)\n",
              "               ToTensor()\n",
              "               Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))\n",
              "           )"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6McrCvV9eIa",
        "outputId": "cb44fe6a-143f-446b-c9dd-be44e1b73f8e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainset.data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8u5tPd9-R5R",
        "outputId": "ed54c709-210c-4b6a-9b7d-8b669b7de087"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "testset = torchvision.datasets.CIFAR10(\"data\", train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy1hsPZW-aaq",
        "outputId": "cf0d4a0d-ef19-4f5d-bff7-1aadb99fe2f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset CIFAR10\n",
              "    Number of datapoints: 10000\n",
              "    Root location: data\n",
              "    Split: Test\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               Resize(size=32, interpolation=bilinear, max_size=None, antialias=warn)\n",
              "               ToTensor()\n",
              "               Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))\n",
              "           )"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xqM0m9TIT0L",
        "outputId": "173901a3-f9e5-4b5f-ac51-71fd5559bf8b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000, 32, 32, 3)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testset.data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOWzwhghHai7"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=False)\n",
        "\n",
        "\n",
        "def train(net, epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    optimizer = optim.Adam(net.parameters(), lr=1e-4)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        # Train with amp\n",
        "        with torch.cuda.amp.autocast(enabled=False):\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if batch_idx == len(trainloader) - 1:\n",
        "          print('train', batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "            % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    return train_loss/(batch_idx+1)\n",
        "\n",
        "\n",
        "def test(net, epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            if batch_idx == len(testloader) - 1:\n",
        "              print('test', batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "    acc = 100.*correct/total\n",
        "\n",
        "    return test_loss, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g52ImgW8yWUO"
      },
      "outputs": [],
      "source": [
        "def pair(t):\n",
        "  return t if isinstance(t, tuple) else (t, t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsK7jAV-AGQF"
      },
      "source": [
        "#### Vision Transformer\n",
        "This is an implementation of the original Vision Transformer in PyTorch found [here](https://github.com/kentaroy47/vision-transformers-cifar10/blob/main/models/vit.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujZlqJgTya3S"
      },
      "outputs": [],
      "source": [
        "class PreNorm(nn.Module):\n",
        "  def __init__(self, dim, fn):\n",
        "    super().__init__()\n",
        "    self.norm = nn.LayerNorm(dim)\n",
        "    self.fn = fn\n",
        "\n",
        "  def forward(self, x, **kwargs):\n",
        "    return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, dim, hidden_dim, dropout = 0.):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "      nn.Linear(dim, hidden_dim),\n",
        "      nn.GELU(),\n",
        "      nn.Dropout(dropout),\n",
        "      nn.Linear(hidden_dim, dim),\n",
        "      nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kWnzJxXynMa"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
        "    super().__init__()\n",
        "    inner_dim = dim_head * heads\n",
        "    project_out = not (heads == 1 and dim_head == dim)\n",
        "\n",
        "    self.heads = heads\n",
        "    self.scale = dim_head ** -0.5\n",
        "\n",
        "    self.attend = nn.Softmax(dim = -1)\n",
        "    self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "\n",
        "    self.to_out = nn.Sequential(\n",
        "      nn.Linear(inner_dim, dim),\n",
        "      nn.Dropout(dropout)\n",
        "      ) if project_out else nn.Identity()\n",
        "  def forward(self, x):\n",
        "    qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
        "    q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
        "\n",
        "    dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "\n",
        "    attn = self.attend(dots)\n",
        "\n",
        "    out = torch.matmul(attn, v)\n",
        "    out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "    return self.to_out(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uFAjWgDy6Gx"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList([])\n",
        "    for _ in range(depth):\n",
        "      self.layers.append(nn.ModuleList([\n",
        "        PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
        "        PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
        "        ]))\n",
        "  def forward(self, x):\n",
        "    for attn, ff in self.layers:\n",
        "      x = attn(x) + x\n",
        "      x = ff(x) + x\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dO4pXdykzDgC"
      },
      "outputs": [],
      "source": [
        "class ViT(nn.Module):\n",
        "  def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
        "    super().__init__()\n",
        "    image_height, image_width = pair(image_size)\n",
        "    patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "    assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "    num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "    patch_dim = channels * patch_height * patch_width\n",
        "    assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "\n",
        "    self.to_patch_embedding = nn.Sequential(\n",
        "      Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width), # from 512,3,32,32 -> 512, 64, 48\n",
        "      nn.Linear(patch_dim, dim),\n",
        "      )\n",
        "\n",
        "    self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "    self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "    self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "    self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "\n",
        "    self.pool = pool\n",
        "    self.to_latent = nn.Identity()\n",
        "\n",
        "    self.mlp_head = nn.Sequential(\n",
        "        nn.LayerNorm(dim),\n",
        "        nn.Linear(dim, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, img):\n",
        "    x = self.to_patch_embedding(img)\n",
        "    b, n, _ = x.shape\n",
        "\n",
        "    cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
        "    x = torch.cat((cls_tokens, x), dim=1)\n",
        "    x += self.pos_embedding[:, :(n + 1)]\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    x = self.transformer(x)\n",
        "\n",
        "    x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
        "\n",
        "    x = self.to_latent(x)\n",
        "    return self.mlp_head(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elEoVGVBKYEi"
      },
      "source": [
        "##### Results with ViT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5Ihp10UJoQJ"
      },
      "outputs": [],
      "source": [
        "net = ViT(\n",
        "    image_size = size,\n",
        "    patch_size = 4,\n",
        "    num_classes = 10,\n",
        "    dim = 512,\n",
        "    depth = 6,\n",
        "    heads = 8,\n",
        "    mlp_dim = 512,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKcMCZpKLOrQ"
      },
      "outputs": [],
      "source": [
        "net = torch.nn.DataParallel(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "magbvIcSKbTJ",
        "outputId": "4873dda0-b88a-4a63-e1f3-0f990f2b2de3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0\n",
            "train 97 98 Loss: 2.074 | Acc: 22.938% (11469/50000)\n",
            "test 99 100 Loss: 1.830 | Acc: 33.630% (3363/10000)\n",
            "\n",
            "Epoch: 1\n",
            "train 97 98 Loss: 1.830 | Acc: 33.116% (16558/50000)\n",
            "test 99 100 Loss: 1.683 | Acc: 39.430% (3943/10000)\n",
            "\n",
            "Epoch: 2\n",
            "train 97 98 Loss: 1.732 | Acc: 36.692% (18346/50000)\n",
            "test 99 100 Loss: 1.628 | Acc: 41.600% (4160/10000)\n",
            "\n",
            "Epoch: 3\n",
            "train 97 98 Loss: 1.679 | Acc: 38.794% (19397/50000)\n",
            "test 99 100 Loss: 1.570 | Acc: 43.980% (4398/10000)\n",
            "\n",
            "Epoch: 4\n",
            "train 97 98 Loss: 1.645 | Acc: 40.314% (20157/50000)\n",
            "test 99 100 Loss: 1.555 | Acc: 44.570% (4457/10000)\n",
            "\n",
            "Epoch: 5\n",
            "train 97 98 Loss: 1.623 | Acc: 41.166% (20583/50000)\n",
            "test 99 100 Loss: 1.518 | Acc: 46.090% (4609/10000)\n",
            "\n",
            "Epoch: 6\n",
            "train 97 98 Loss: 1.601 | Acc: 41.976% (20988/50000)\n",
            "test 99 100 Loss: 1.511 | Acc: 46.660% (4666/10000)\n",
            "\n",
            "Epoch: 7\n",
            "train 97 98 Loss: 1.583 | Acc: 42.636% (21318/50000)\n",
            "test 99 100 Loss: 1.487 | Acc: 47.390% (4739/10000)\n",
            "\n",
            "Epoch: 8\n",
            "train 97 98 Loss: 1.570 | Acc: 43.128% (21564/50000)\n",
            "test 99 100 Loss: 1.481 | Acc: 47.350% (4735/10000)\n",
            "\n",
            "Epoch: 9\n",
            "train 97 98 Loss: 1.559 | Acc: 43.380% (21690/50000)\n",
            "test 99 100 Loss: 1.464 | Acc: 47.740% (4774/10000)\n",
            "\n",
            "Epoch: 10\n",
            "train 97 98 Loss: 1.545 | Acc: 43.952% (21976/50000)\n",
            "test 99 100 Loss: 1.448 | Acc: 48.670% (4867/10000)\n",
            "\n",
            "Epoch: 11\n",
            "train 97 98 Loss: 1.538 | Acc: 44.392% (22196/50000)\n",
            "test 99 100 Loss: 1.444 | Acc: 49.080% (4908/10000)\n",
            "\n",
            "Epoch: 12\n",
            "train 97 98 Loss: 1.526 | Acc: 44.988% (22494/50000)\n",
            "test 99 100 Loss: 1.431 | Acc: 49.220% (4922/10000)\n",
            "\n",
            "Epoch: 13\n",
            "train 97 98 Loss: 1.518 | Acc: 45.024% (22512/50000)\n",
            "test 99 100 Loss: 1.434 | Acc: 49.010% (4901/10000)\n",
            "\n",
            "Epoch: 14\n",
            "train 97 98 Loss: 1.501 | Acc: 45.690% (22845/50000)\n",
            "test 99 100 Loss: 1.404 | Acc: 49.980% (4998/10000)\n",
            "\n",
            "Epoch: 15\n",
            "train 97 98 Loss: 1.492 | Acc: 45.946% (22973/50000)\n",
            "test 99 100 Loss: 1.405 | Acc: 50.390% (5039/10000)\n",
            "\n",
            "Epoch: 16\n",
            "train 97 98 Loss: 1.490 | Acc: 45.956% (22978/50000)\n",
            "test 99 100 Loss: 1.398 | Acc: 50.440% (5044/10000)\n",
            "\n",
            "Epoch: 17\n",
            "train 97 98 Loss: 1.476 | Acc: 46.396% (23198/50000)\n",
            "test 99 100 Loss: 1.411 | Acc: 50.310% (5031/10000)\n",
            "\n",
            "Epoch: 18\n",
            "train 97 98 Loss: 1.470 | Acc: 46.994% (23497/50000)\n",
            "test 99 100 Loss: 1.394 | Acc: 50.940% (5094/10000)\n",
            "\n",
            "Epoch: 19\n",
            "train 97 98 Loss: 1.465 | Acc: 47.048% (23524/50000)\n",
            "test 99 100 Loss: 1.373 | Acc: 51.650% (5165/10000)\n",
            "\n",
            "Epoch: 20\n",
            "train 97 98 Loss: 1.454 | Acc: 47.268% (23634/50000)\n",
            "test 99 100 Loss: 1.361 | Acc: 51.630% (5163/10000)\n",
            "\n",
            "Epoch: 21\n",
            "train 97 98 Loss: 1.441 | Acc: 47.932% (23966/50000)\n",
            "test 99 100 Loss: 1.343 | Acc: 52.440% (5244/10000)\n",
            "\n",
            "Epoch: 22\n",
            "train 97 98 Loss: 1.438 | Acc: 48.078% (24039/50000)\n",
            "test 99 100 Loss: 1.371 | Acc: 51.790% (5179/10000)\n",
            "\n",
            "Epoch: 23\n",
            "train 97 98 Loss: 1.426 | Acc: 48.432% (24216/50000)\n",
            "test 99 100 Loss: 1.341 | Acc: 52.750% (5275/10000)\n",
            "\n",
            "Epoch: 24\n",
            "train 97 98 Loss: 1.422 | Acc: 48.716% (24358/50000)\n",
            "test 99 100 Loss: 1.368 | Acc: 52.450% (5245/10000)\n"
          ]
        }
      ],
      "source": [
        "for i in range(25):\n",
        "  train(net, i)\n",
        "  test(net, i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZjBrxV2CABK"
      },
      "source": [
        "#### Adding Convolutions - 1: On the patches\n",
        "Instead of having a Linear Projection from the patches being fed into the Transformer encoder, we can convolve over the patches then feed them into the encoder. We can simulate this by having both kernel size and stride be equal to the size of the patches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ic54W7AwCGKB"
      },
      "outputs": [],
      "source": [
        "class ViTConv1(nn.Module):\n",
        "  def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
        "    super().__init__()\n",
        "    image_height, image_width = pair(image_size)\n",
        "    patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "    assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "    num_patches = (image_height // patch_height) * (image_width // patch_width) # 64\n",
        "    patch_dim = channels * patch_height * patch_width # 48\n",
        "    assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "\n",
        "    self.to_patch_embedding = nn.Sequential(\n",
        "      nn.Conv2d(3, 3, kernel_size=patch_height, stride=patch_height), # -> 512, 3, 8, 8\n",
        "      Rearrange('b c h w -> b (h w) c'), # -> 512 64 3\n",
        "      nn.Linear(3, dim),\n",
        "      )\n",
        "\n",
        "    self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "    self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "    self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "    self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "\n",
        "    self.pool = pool\n",
        "    self.to_latent = nn.Identity()\n",
        "\n",
        "    self.mlp_head = nn.Sequential(\n",
        "        nn.LayerNorm(dim),\n",
        "        nn.Linear(dim, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, img):\n",
        "    x = self.to_patch_embedding(img)\n",
        "    b, n, _ = x.shape\n",
        "    cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
        "    x = torch.cat((cls_tokens, x), dim=1)\n",
        "    x += self.pos_embedding[:, :(n + 1)]\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    x = self.transformer(x)\n",
        "\n",
        "    x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
        "\n",
        "    x = self.to_latent(x)\n",
        "    return self.mlp_head(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_rSD24rPP6C"
      },
      "outputs": [],
      "source": [
        "net2 = ViTConv1(\n",
        "    image_size = size,\n",
        "    patch_size = 4,\n",
        "    num_classes = 10,\n",
        "    dim = 512,\n",
        "    depth = 6,\n",
        "    heads = 8,\n",
        "    mlp_dim = 512,W\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        ")\n",
        "net2 = torch.nn.DataParallel(net2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvvU2d4yPZn2",
        "outputId": "67df6961-072a-4787-9ebc-53549dc6cdd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0\n",
            "train 97 98 Loss: 2.126 | Acc: 20.312% (10156/50000)\n",
            "test 99 100 Loss: 1.916 | Acc: 29.790% (2979/10000)\n",
            "\n",
            "Epoch: 1\n",
            "train 97 98 Loss: 1.934 | Acc: 28.966% (14483/50000)\n",
            "test 99 100 Loss: 1.818 | Acc: 34.740% (3474/10000)\n",
            "\n",
            "Epoch: 2\n",
            "train 97 98 Loss: 1.858 | Acc: 32.516% (16258/50000)\n",
            "test 99 100 Loss: 1.745 | Acc: 37.620% (3762/10000)\n",
            "\n",
            "Epoch: 3\n",
            "train 97 98 Loss: 1.798 | Acc: 34.682% (17341/50000)\n",
            "test 99 100 Loss: 1.700 | Acc: 38.910% (3891/10000)\n",
            "\n",
            "Epoch: 4\n",
            "train 97 98 Loss: 1.755 | Acc: 36.468% (18234/50000)\n",
            "test 99 100 Loss: 1.652 | Acc: 40.520% (4052/10000)\n",
            "\n",
            "Epoch: 5\n",
            "train 97 98 Loss: 1.727 | Acc: 37.000% (18500/50000)\n",
            "test 99 100 Loss: 1.634 | Acc: 41.150% (4115/10000)\n",
            "\n",
            "Epoch: 6\n",
            "train 97 98 Loss: 1.705 | Acc: 38.270% (19135/50000)\n",
            "test 99 100 Loss: 1.616 | Acc: 42.020% (4202/10000)\n",
            "\n",
            "Epoch: 7\n",
            "train 97 98 Loss: 1.685 | Acc: 39.106% (19553/50000)\n",
            "test 99 100 Loss: 1.581 | Acc: 42.790% (4279/10000)\n",
            "\n",
            "Epoch: 8\n",
            "train 97 98 Loss: 1.666 | Acc: 39.432% (19716/50000)\n",
            "test 99 100 Loss: 1.586 | Acc: 43.180% (4318/10000)\n",
            "\n",
            "Epoch: 9\n",
            "train 97 98 Loss: 1.654 | Acc: 40.178% (20089/50000)\n",
            "test 99 100 Loss: 1.555 | Acc: 44.000% (4400/10000)\n",
            "\n",
            "Epoch: 10\n",
            "train 97 98 Loss: 1.636 | Acc: 40.636% (20318/50000)\n",
            "test 99 100 Loss: 1.534 | Acc: 44.710% (4471/10000)\n",
            "\n",
            "Epoch: 11\n",
            "train 97 98 Loss: 1.618 | Acc: 41.428% (20714/50000)\n",
            "test 99 100 Loss: 1.531 | Acc: 45.450% (4545/10000)\n",
            "\n",
            "Epoch: 12\n",
            "train 97 98 Loss: 1.611 | Acc: 41.828% (20914/50000)\n",
            "test 99 100 Loss: 1.516 | Acc: 45.770% (4577/10000)\n",
            "\n",
            "Epoch: 13\n",
            "train 97 98 Loss: 1.600 | Acc: 41.842% (20921/50000)\n",
            "test 99 100 Loss: 1.510 | Acc: 46.130% (4613/10000)\n",
            "\n",
            "Epoch: 14\n",
            "train 97 98 Loss: 1.584 | Acc: 42.742% (21371/50000)\n",
            "test 99 100 Loss: 1.490 | Acc: 46.970% (4697/10000)\n",
            "\n",
            "Epoch: 15\n",
            "train 97 98 Loss: 1.576 | Acc: 42.756% (21378/50000)\n",
            "test 99 100 Loss: 1.479 | Acc: 47.120% (4712/10000)\n",
            "\n",
            "Epoch: 16\n",
            "train 97 98 Loss: 1.564 | Acc: 43.576% (21788/50000)\n",
            "test 99 100 Loss: 1.479 | Acc: 47.280% (4728/10000)\n",
            "\n",
            "Epoch: 17\n",
            "train 97 98 Loss: 1.557 | Acc: 43.756% (21878/50000)\n",
            "test 99 100 Loss: 1.480 | Acc: 47.510% (4751/10000)\n",
            "\n",
            "Epoch: 18\n",
            "train 97 98 Loss: 1.548 | Acc: 43.806% (21903/50000)\n",
            "test 99 100 Loss: 1.478 | Acc: 47.390% (4739/10000)\n",
            "\n",
            "Epoch: 19\n",
            "train 97 98 Loss: 1.547 | Acc: 44.242% (22121/50000)\n",
            "test 99 100 Loss: 1.461 | Acc: 48.040% (4804/10000)\n",
            "\n",
            "Epoch: 20\n",
            "train 97 98 Loss: 1.537 | Acc: 44.268% (22134/50000)\n",
            "test 99 100 Loss: 1.449 | Acc: 48.100% (4810/10000)\n",
            "\n",
            "Epoch: 21\n",
            "train 97 98 Loss: 1.528 | Acc: 44.936% (22468/50000)\n",
            "test 99 100 Loss: 1.458 | Acc: 48.340% (4834/10000)\n",
            "\n",
            "Epoch: 22\n",
            "train 97 98 Loss: 1.522 | Acc: 44.856% (22428/50000)\n",
            "test 99 100 Loss: 1.438 | Acc: 49.210% (4921/10000)\n",
            "\n",
            "Epoch: 23\n",
            "train 97 98 Loss: 1.514 | Acc: 45.518% (22759/50000)\n",
            "test 99 100 Loss: 1.438 | Acc: 49.090% (4909/10000)\n",
            "\n",
            "Epoch: 24\n",
            "train 97 98 Loss: 1.509 | Acc: 45.246% (22623/50000)\n",
            "test 99 100 Loss: 1.438 | Acc: 48.660% (4866/10000)\n"
          ]
        }
      ],
      "source": [
        "for i in range(25):\n",
        "  train(net2, i)\n",
        "  test(net2, i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFsWkjNjjlQr"
      },
      "source": [
        "#### Adding Convolutions - 2:\n",
        "We can expand on the idea of applying convolutions on the raw image and then applying some embedding/tokenization process. Here we will replace the ViT patch embeddings module with a new module that convolves the images and pools them. We then split this convolved form as part of our embeddings.\n",
        "\n",
        "We also modify the MLP/Feed Forward network in the original ViT architecture by incorporating both point-wise and depth-wise convolutions, as well as use a form of attention on the class token from the vision transformer at the end of the transformer encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaYbhxoAGzjD"
      },
      "outputs": [],
      "source": [
        "class ImageToEmbedding(nn.Module):\n",
        "  def __init__(self, in_chans=3, out_chans=64, kernel_size=7, stride=2):\n",
        "    super(ImageToEmbedding, self).__init__()\n",
        "    self.conv = nn.Conv2d(in_chans, out_chans, kernel_size=kernel_size, stride=stride,\n",
        "                          padding=kernel_size // 2, bias=False)\n",
        "    self.bn = nn.BatchNorm2d(out_chans)\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = self.bn(x)\n",
        "    x = self.maxpool(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jK03kR1UQh2e"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, dim, hidden_dim, dropout = 0.):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "      nn.Linear(dim, hidden_dim),\n",
        "      nn.GELU(),\n",
        "      nn.Dropout(dropout),\n",
        "      nn.Linear(hidden_dim, dim),\n",
        "      nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M76ipwBwr0HV"
      },
      "outputs": [],
      "source": [
        "class ConvFeedForward(nn.Module):\n",
        "  def __init__(self, in_features, hidden_features=None, out_features=None, drop=0., kernel_size=3):\n",
        "    super().__init__()\n",
        "    out_features = out_features or in_features\n",
        "    hidden_features = hidden_features or in_features\n",
        "    # pointwise\n",
        "    self.conv1 = nn.Conv2d(in_features, hidden_features, kernel_size=1, stride=1, padding=0)\n",
        "    # depthwise\n",
        "    self.conv2 = nn.Conv2d(hidden_features, hidden_features, kernel_size=kernel_size, stride=1, padding=(kernel_size - 1) // 2, groups=hidden_features)\n",
        "    # pointwise\n",
        "    self.conv3 = nn.Conv2d(hidden_features, out_features, kernel_size=1, stride=1, padding=0)\n",
        "    self.act = nn.GELU()\n",
        "\n",
        "    self.bn1 = nn.BatchNorm2d(hidden_features)\n",
        "    self.bn2 = nn.BatchNorm2d(hidden_features)\n",
        "    self.bn3 = nn.BatchNorm2d(out_features)\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, n, k = x.size()\n",
        "    cls_token, tokens = torch.split(x, [1, n - 1], dim=1)\n",
        "    x = tokens.reshape(b, int(math.sqrt(n - 1)), int(math.sqrt(n - 1)), k).permute(0, 3, 1, 2)\n",
        "\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.act(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    x = self.act(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.bn3(x)\n",
        "\n",
        "    tokens = x.flatten(2).permute(0, 2, 1)\n",
        "    out = torch.cat((cls_token, tokens), dim=1)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lB6muGJ0wGOY"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, dim, num_heads=8, attn_drop=0., proj_drop=0.):\n",
        "    super().__init__()\n",
        "    self.num_heads = num_heads\n",
        "    head_dim = dim // num_heads\n",
        "\n",
        "    self.scale = head_dim ** -0.5\n",
        "\n",
        "    self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "    self.attn_drop = nn.Dropout(attn_drop)\n",
        "    self.proj = nn.Linear(dim, dim)\n",
        "    self.proj_drop = nn.Dropout(proj_drop)\n",
        "    self.attention_map = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, N, C = x.shape\n",
        "    qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "    q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "    attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "    attn = attn.softmax(dim=-1)\n",
        "    # self.attention_map = attn\n",
        "    attn = self.attn_drop(attn)\n",
        "\n",
        "    x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "    x = self.proj(x)\n",
        "    x = self.proj_drop(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dj6KmUtA-1DF"
      },
      "outputs": [],
      "source": [
        "class LayerAttention(Attention):\n",
        "  def __init__(self, dim, num_heads=8, attn_drop=0., proj_drop=0.):\n",
        "    super(LayerAttention, self).__init__(dim, num_heads, attn_drop, proj_drop)\n",
        "    self.dim = dim\n",
        "\n",
        "  def forward(self, x):\n",
        "    q_weight = self.qkv.weight[:self.dim, :]\n",
        "    q_bias = None\n",
        "    kv_weight = self.qkv.weight[self.dim:, :]\n",
        "    kv_bias = None\n",
        "\n",
        "    B, N, C = x.shape\n",
        "    _, last_token = torch.split(x, [N-1, 1], dim=1)\n",
        "\n",
        "    q = F.linear(last_token, q_weight, q_bias).reshape(B, 1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
        "    kv = F.linear(x, kv_weight, kv_bias).reshape(B, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "    k, v = kv[0], kv[1]\n",
        "\n",
        "    attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "    attn = attn.softmax(dim=-1)\n",
        "    # self.attention_map = attn\n",
        "    attn = self.attn_drop(attn)\n",
        "\n",
        "    x = (attn @ v).transpose(1, 2).reshape(B, 1, C)\n",
        "    x = self.proj(x)\n",
        "    x = self.proj_drop(x)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqHq32-Zt6oK"
      },
      "outputs": [],
      "source": [
        "class ConvTransformerBlock(nn.Module):\n",
        "  def __init__(self, dim, num_heads, mlp_ratio=4, drop=0., attn_drop=0., kernel_size=3, use_cff=True):\n",
        "    super().__init__()\n",
        "    self.norm1 = nn.LayerNorm(dim)\n",
        "    self.norm2 = nn.LayerNorm(dim)\n",
        "    mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "    self.use_cff = use_cff\n",
        "    if self.use_cff:\n",
        "      self.attn = Attention(dim, num_heads=num_heads, attn_drop=attn_drop, proj_drop=drop)\n",
        "      self.ff = ConvFeedForward(in_features=dim, hidden_features=mlp_hidden_dim, drop=drop, kernel_size=kernel_size)\n",
        "    else:  # use layer attention\n",
        "      self.attn = LayerAttention(dim, num_heads=num_heads, attn_drop=attn_drop, proj_drop=drop)\n",
        "      self.ff = FeedForward(dim, mlp_hidden_dim, dropout=drop)\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.use_cff:\n",
        "      x = x + self.attn(self.norm1(x))\n",
        "      x = x + self.ff(self.norm2(x))\n",
        "      return x, x[:, 0]\n",
        "    else:\n",
        "      _, last_token = torch.split(x, [x.size(1)-1, 1], dim=1)\n",
        "      x = last_token + self.attn(self.norm1(x))\n",
        "      x = x + self.ff(self.norm2(x))\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWyRxS4DUeFr"
      },
      "outputs": [],
      "source": [
        "class HybridEmbed(nn.Module):\n",
        "  def __init__(self, backbone, img_size=32, patch_size=16, feature_size=None, in_chans=3, embed_dim=512):\n",
        "    super().__init__()\n",
        "    assert isinstance(backbone, nn.Module)\n",
        "    img_size = pair(img_size)\n",
        "    self.img_size = img_size\n",
        "    self.backbone = backbone\n",
        "    if feature_size is None:\n",
        "      with torch.no_grad():\n",
        "        training = backbone.training\n",
        "        if training:\n",
        "          backbone.eval()\n",
        "        o = self.backbone(torch.zeros(1, in_chans, img_size[0], img_size[1]))\n",
        "        if isinstance(o, (list, tuple)):\n",
        "          o = o[-1]\n",
        "        feature_size = o.shape[-2:]\n",
        "        feature_dim = o.shape[1]\n",
        "        backbone.train(training)\n",
        "    else:\n",
        "      feature_size = pair(feature_size)\n",
        "      feature_dim = self.backbone.feature_info.channels()[-1]\n",
        "\n",
        "    self.num_patches = (feature_size[0] // patch_size) * (feature_size[1] // patch_size)\n",
        "    self.proj = nn.Conv2d(feature_dim, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.backbone(x)\n",
        "    if isinstance(x, (list, tuple)):\n",
        "      x = x[-1]\n",
        "    x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtXfeTnQT6pz"
      },
      "outputs": [],
      "source": [
        "class ViTConv2(nn.Module):\n",
        "  def __init__(self, img_size=32, patch_size=16, in_chans=3, num_classes=10, embed_dim=512, depth=6, num_heads=8,\n",
        "               mlp_ratio=4., drop_rate=0., attn_drop_rate=0., drop_path_rate=0., cff_local_size=3):\n",
        "    super().__init__()\n",
        "    self.num_classes = num_classes\n",
        "    self.num_features = self.embed_dim = embed_dim\n",
        "\n",
        "    self.i2thead = ImageToEmbedding()\n",
        "    self.i2t = HybridEmbed(self.i2thead, img_size=img_size, patch_size=patch_size)\n",
        "\n",
        "    num_patches = self.i2t.num_patches\n",
        "    self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "    self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "    self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "    dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
        "    self.blocks = nn.ModuleList([\n",
        "        ConvTransformerBlock(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
        "                                 drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                                 kernel_size=cff_local_size)\n",
        "        for i in range(depth)])\n",
        "\n",
        "    self.layerattention = ConvTransformerBlock(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
        "                                               drop=drop_rate, attn_drop=attn_drop_rate, use_cff=False)\n",
        "    self.pos_layer_embed = nn.Parameter(torch.zeros(1, depth, embed_dim))\n",
        "\n",
        "    self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "\n",
        "    # Classifier head\n",
        "    self.mlp_head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "  def forward_features(self, x):\n",
        "    B = x.shape[0]\n",
        "    x = self.i2t(x)\n",
        "\n",
        "    cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "    x = torch.cat((cls_tokens, x), dim=1)\n",
        "    x = x + self.pos_embed\n",
        "    x = self.pos_drop(x)\n",
        "\n",
        "    cls_token_list = []\n",
        "    for blk in self.blocks:\n",
        "      x, curr_cls_token = blk(x)\n",
        "      cls_token_list.append(curr_cls_token)\n",
        "\n",
        "    all_cls_token = torch.stack(cls_token_list, dim=1)\n",
        "    all_cls_token = all_cls_token + self.pos_layer_embed\n",
        "\n",
        "    last_cls_token = self.layerattention(all_cls_token)\n",
        "    last_cls_token = self.norm(last_cls_token)\n",
        "\n",
        "    return last_cls_token.view(B, -1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.forward_features(x)\n",
        "    x = self.mlp_head(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IEFBFygS1wc"
      },
      "outputs": [],
      "source": [
        "net3 = ViTConv2(img_size=size, patch_size=4, drop_rate = 0.1)\n",
        "net3 = torch.nn.DataParallel(net3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeySZUVgWqib",
        "outputId": "1262e1c7-6a07-414d-af45-ec2b72e1744b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0\n",
            "train 97 98 Loss: 1.650 | Acc: 39.978% (19989/50000)\n",
            "test 99 100 Loss: 1.336 | Acc: 52.210% (5221/10000)\n",
            "\n",
            "Epoch: 1\n",
            "train 97 98 Loss: 1.333 | Acc: 51.586% (25793/50000)\n",
            "test 99 100 Loss: 1.195 | Acc: 56.740% (5674/10000)\n",
            "\n",
            "Epoch: 2\n",
            "train 97 98 Loss: 1.212 | Acc: 56.600% (28300/50000)\n",
            "test 99 100 Loss: 1.066 | Acc: 62.070% (6207/10000)\n",
            "\n",
            "Epoch: 3\n",
            "train 97 98 Loss: 1.129 | Acc: 59.644% (29822/50000)\n",
            "test 99 100 Loss: 1.031 | Acc: 63.770% (6377/10000)\n",
            "\n",
            "Epoch: 4\n",
            "train 97 98 Loss: 1.077 | Acc: 61.148% (30574/50000)\n",
            "test 99 100 Loss: 0.969 | Acc: 65.620% (6562/10000)\n",
            "\n",
            "Epoch: 5\n",
            "train 97 98 Loss: 1.020 | Acc: 63.546% (31773/50000)\n",
            "test 99 100 Loss: 0.951 | Acc: 66.470% (6647/10000)\n",
            "\n",
            "Epoch: 6\n",
            "train 97 98 Loss: 0.970 | Acc: 65.470% (32735/50000)\n",
            "test 99 100 Loss: 0.940 | Acc: 67.090% (6709/10000)\n",
            "\n",
            "Epoch: 7\n",
            "train 97 98 Loss: 0.933 | Acc: 66.740% (33370/50000)\n",
            "test 99 100 Loss: 0.871 | Acc: 68.730% (6873/10000)\n",
            "\n",
            "Epoch: 8\n",
            "train 97 98 Loss: 0.892 | Acc: 68.408% (34204/50000)\n",
            "test 99 100 Loss: 0.850 | Acc: 70.250% (7025/10000)\n",
            "\n",
            "Epoch: 9\n",
            "train 97 98 Loss: 0.863 | Acc: 69.154% (34577/50000)\n",
            "test 99 100 Loss: 0.842 | Acc: 70.730% (7073/10000)\n",
            "\n",
            "Epoch: 10\n",
            "train 97 98 Loss: 0.835 | Acc: 70.222% (35111/50000)\n",
            "test 99 100 Loss: 0.834 | Acc: 71.380% (7138/10000)\n",
            "\n",
            "Epoch: 11\n",
            "train 97 98 Loss: 0.801 | Acc: 71.350% (35675/50000)\n",
            "test 99 100 Loss: 0.805 | Acc: 72.360% (7236/10000)\n",
            "\n",
            "Epoch: 12\n",
            "train 97 98 Loss: 0.784 | Acc: 71.998% (35999/50000)\n",
            "test 99 100 Loss: 0.791 | Acc: 72.730% (7273/10000)\n",
            "\n",
            "Epoch: 13\n",
            "train 97 98 Loss: 0.755 | Acc: 73.312% (36656/50000)\n",
            "test 99 100 Loss: 0.757 | Acc: 73.990% (7399/10000)\n",
            "\n",
            "Epoch: 14\n",
            "train 97 98 Loss: 0.730 | Acc: 74.084% (37042/50000)\n",
            "test 99 100 Loss: 0.772 | Acc: 73.140% (7314/10000)\n",
            "\n",
            "Epoch: 15\n",
            "train 97 98 Loss: 0.715 | Acc: 74.584% (37292/50000)\n",
            "test 99 100 Loss: 0.734 | Acc: 75.130% (7513/10000)\n",
            "\n",
            "Epoch: 16\n",
            "train 97 98 Loss: 0.691 | Acc: 75.446% (37723/50000)\n",
            "test 99 100 Loss: 0.722 | Acc: 75.140% (7514/10000)\n",
            "\n",
            "Epoch: 17\n",
            "train 97 98 Loss: 0.675 | Acc: 76.074% (38037/50000)\n",
            "test 99 100 Loss: 0.743 | Acc: 74.240% (7424/10000)\n",
            "\n",
            "Epoch: 18\n",
            "train 97 98 Loss: 0.653 | Acc: 77.116% (38558/50000)\n",
            "test 99 100 Loss: 0.728 | Acc: 75.490% (7549/10000)\n",
            "\n",
            "Epoch: 19\n",
            "train 97 98 Loss: 0.630 | Acc: 77.848% (38924/50000)\n",
            "test 99 100 Loss: 0.736 | Acc: 74.810% (7481/10000)\n",
            "\n",
            "Epoch: 20\n",
            "train 97 98 Loss: 0.619 | Acc: 78.228% (39114/50000)\n",
            "test 99 100 Loss: 0.762 | Acc: 75.000% (7500/10000)\n",
            "\n",
            "Epoch: 21\n",
            "train 97 98 Loss: 0.599 | Acc: 78.638% (39319/50000)\n",
            "test 99 100 Loss: 0.713 | Acc: 75.760% (7576/10000)\n",
            "\n",
            "Epoch: 22\n",
            "train 97 98 Loss: 0.580 | Acc: 79.298% (39649/50000)\n",
            "test 99 100 Loss: 0.702 | Acc: 76.500% (7650/10000)\n",
            "\n",
            "Epoch: 23\n",
            "train 97 98 Loss: 0.567 | Acc: 79.944% (39972/50000)\n",
            "test 99 100 Loss: 0.717 | Acc: 76.400% (7640/10000)\n",
            "\n",
            "Epoch: 24\n",
            "train 97 98 Loss: 0.552 | Acc: 80.244% (40122/50000)\n",
            "test 99 100 Loss: 0.713 | Acc: 76.520% (7652/10000)\n"
          ]
        }
      ],
      "source": [
        "for i in range(25):\n",
        "  train(net3, i)\n",
        "  test(net3, i)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
